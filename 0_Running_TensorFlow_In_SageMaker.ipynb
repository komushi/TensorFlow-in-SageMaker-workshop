{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Keras Sequential Model\n",
    "This notebook shows how to train and host a Keras Sequential model on SageMaker. The model used for this notebook is a simple deep CNN that was extracted from [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) is one of the most popular machine learning datasets. It consists of 60,000 32x32 images belonging to 10 different classes (6,000 images per class). Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n",
    "\n",
    "In this tutorial, we will train a deep CNN to recognize these images.\n",
    "\n",
    "We'll compare trainig with file mode, pipe mode datasets and distributed training with Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sagemaker-experiments sagemaker boto3 awscli --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                         2.3.0              \n",
      "tensorflow-datasets                3.2.1              \n",
      "tensorflow-estimator               2.3.0              \n",
      "tensorflow-gpu                     2.3.0              \n",
      "tensorflow-metadata                0.24.0             \n",
      "tensorflow-serving-api             2.3.0              \n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U tensorflow-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-estimator               2.1.0              \n",
      "tensorflow-gpu                     2.1.0              \n",
      "tensorflow-serving-api             2.1.0              \n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::042083552617:role/service-role/AmazonSageMaker-ExecutionRole-20180815T111764\n",
      "Session(region_name='ap-northeast-1')\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(sess.boto_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-1-042083552617/tensorflow_datasets\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()\n",
    "prefix = 'tensorflow_datasets'\n",
    "\n",
    "s3_data_dir = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "print(s3_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ai2_arc',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'arc',\n",
       " 'bair_robot_pushing_small',\n",
       " 'beans',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'blimp',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'i_naturalist2017',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mctaco',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'qa4mre',\n",
       " 'quickdraw_bitmap',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'robonet',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vctk',\n",
       " 'vgg_face2',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'winogrande',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'xnli',\n",
       " 'xsum',\n",
       " 'yelp_polarity_reviews']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset iris/2.0.0 (download: 4.44 KiB, generated: Unknown size, total: 4.44 KiB) to /home/ec2-user/tensorflow_datasets/iris/2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a4f2a5d52482384f468f36b9a5e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d3b33a634d48c2a5db78f066825775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a6477178d2406797d7bd9a23eb2be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ec2-user/tensorflow_datasets/iris/2.0.0.incomplete38V1KL/iris-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36536d87c044da788a7fd879c4e316d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset iris downloaded and prepared to /home/ec2-user/tensorflow_datasets/iris/2.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# data, info = tfds.load('c4', data_dir=s3_data_dir)\n",
    "# data, info = tfds.load('iris', data_dir=s3_data_dir)\n",
    "\n",
    "data, info = tfds.load(\"iris\",split='train', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(\"iris\", data_dir=s3_data_dir, split='train', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, info = tfds.load('cifar10', split='train', shuffle_files=True, with_info=True, data_dir=s3_data_dir)\n",
    "assert isinstance(ds, tf.data.Dataset)\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mnist, info_mnist = tfds.load('mnist', split='train', with_info=True)\n",
    "assert isinstance(ds_mnist, tf.data.Dataset)\n",
    "print(ds_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = tfds.show_examples(ds, info)\n",
    "fig = tfds.show_examples(ds_mnist, info_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Copy the cifar10 tfrecord datasets from s3://floor28/data/cifar10 to your local notebook\n",
    "\n",
    "You can use the following AWS CLI command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive s3://floor28/data/cifar10 ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script uses arguments for configuration. it requires the following configurations:\n",
    "1. Model_dir - location where it'll save checkpoints and logs\n",
    "2. train, validation, eval - location of the relevant tf records\n",
    "\n",
    "Run the script locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p logs\n",
    "!python training_script/cifar10_keras.py --model_dir ./logs \\\n",
    "                                         --train data/train \\\n",
    "                                         --validation data/validation \\\n",
    "                                         --eval data/eval \\\n",
    "                                         --epochs 1\n",
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Altough the script was running on a SageMaker notebook, you can run the same script on your computer using the same command.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TensorFlow Script Mode\n",
    "For TensorFlow versions 1.11 and later, the Amazon SageMaker Python SDK supports script mode training scripts. Script mode has the following advantages over legacy mode training scripts:\n",
    "\n",
    "* Script mode training scripts are more similar to training scripts you write for TensorFlow in general, so it is easier to modify your existing TensorFlow training scripts to work with Amazon SageMaker.\n",
    "\n",
    "* Script mode supports both Python 2.7- andPython 3.6-compatible source files.\n",
    "\n",
    "* Script mode supports Horovod for distributed training.\n",
    "\n",
    "For information about writing TensorFlow script mode training scripts and using TensorFlow script mode estimators and models with Amazon SageMaker, see https://sagemaker.readthedocs.io/en/stable/using_tf.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your script for training in SageMaker\n",
    "The training script is very similar to a training script you might run outside of SageMaker.\n",
    "SageMaker runs the script with 1 argument, model_dir, an S3 location that is used for logs and artifacts.\n",
    "\n",
    "You can access useful properties about the training environment through various environment variables.\n",
    "In this example, we are sending 3 data channels to the script: Train, Validation, Eval.\n",
    "\n",
    "**Create a copy of the script (training_script/cifar10_keras.py) and save it as training_script/cifar10_keras_sm.py.**\n",
    "\n",
    "In cifar10_keras_sm.py, scroll down to the **if __name__ == '__main__':** section.  \n",
    "Update the train,validation,eval arguments to get the data by default from the relevant environment variable: SM_CHANNEL_TRAIN, SM_CHANNEL_VALIDATION, SM_CHANNEL_EVAL\n",
    "Add the default configuration to the arguments in **cifar10_keras_sm.py**.  \n",
    "The lines should look as following:\n",
    "```python\n",
    "parser.add_argument(\n",
    "        '--train',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default=os.environ.get('SM_CHANNEL_TRAIN'),\n",
    "        help='The directory where the CIFAR-10 input data is stored.')\n",
    "parser.add_argument(\n",
    "        '--validation',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default=os.environ.get('SM_CHANNEL_VALIDATION'),\n",
    "        help='The directory where the CIFAR-10 input data is stored.')\n",
    "parser.add_argument(\n",
    "        '--eval',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default=os.environ.get('SM_CHANNEL_EVAL'),\n",
    "        help='The directory where the CIFAR-10 input data is stored.')\n",
    "```\n",
    "\n",
    "For info see the SageMaker-python-sdk [documentation](https://sagemaker.readthedocs.io/en/stable/using_tf.html#preparing-a-script-mode-training-script)\n",
    "\n",
    "SageMaker will not send the locations as arguments, it'll use environment variables instead.\n",
    "\n",
    "SageMaker send different useful environment variables to your scripts, e.g.:\n",
    "* `SM_MODEL_DIR`: A string that represents the local path where the training job can write the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting. This is different than the model_dir argument passed to your training script which is a S3 location. `SM_MODEL_DIR` is always set to /opt/ml/model.\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "* `SM_OUTPUT_DATA_DIR`: A string that represents the path to the directory to write output artifacts to. Output artifacts might include checkpoints, graphs, and other files to save, but do not include model artifacts. These artifacts are compressed and uploaded to S3 to an S3 bucket with the same prefix as the model artifacts.\n",
    "\n",
    "In this Example, to reduce the network latency. we would like to save the model checkpoints locally, they will be uploaded to S3 at the end of the job.\n",
    "\n",
    "Add the following argument to your script:\n",
    "```python\n",
    "parser.add_argument(\n",
    "        '--model_output_dir',\n",
    "        type=str,\n",
    "        default=os.environ.get('SM_MODEL_DIR'))\n",
    "```\n",
    "Change the ModelCheckPoint line to use to new location:\n",
    "```python\n",
    "checkpoint = ModelCheckpoint(args.model_output_dir + '/checkpoint-{epoch}.h5')\n",
    "```\n",
    "\n",
    "Change the save_model call to use that folder.  \n",
    "From:  \n",
    "```python\n",
    "return save_model(model, args.model_dir)\n",
    "```\n",
    "To:  \n",
    "```python\n",
    "return save_model(model, args.model_output_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your script locally\n",
    "For testing, run the new script with the same command as above, make sure it runs as expected.  \n",
    "Add the new model_output_dir as an argument for the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script locally\n",
    "!mkdir -p logs\n",
    "!python training_script/cifar10_keras_sm.py --model_dir ./logs \\\n",
    "                                         --model_output_dir ./logs \\\n",
    "                                         --train data/train \\\n",
    "                                         --validation data/validation \\\n",
    "                                         --eval data/eval \\\n",
    "                                         --epochs 1\n",
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker local for local testing\n",
    "The local mode in the Amazon SageMaker Python SDK can emulate CPU (single and multi-instance) and GPU (single instance) SageMaker training jobs by changing a single argument in the TensorFlow or MXNet estimators.  To do this, it uses Docker compose and NVIDIA Docker.  It will also pull the Amazon SageMaker TensorFlow container from Amazon ECS.\n",
    "\n",
    "Training in local mode also allows us to easily monitor metrics like GPU consumption to ensure that our code is written properly to take advantage of the hardware we’re using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sagemaker.tensorflow class we will create a new SageMaker TensorFlow job\n",
    "We can use the command to pass different configuration or hyperparameters to the script\n",
    "\n",
    "For info see the [documentation](https://sagemaker.readthedocs.io/en/stable/using_tf.html#training-with-tensorflow-estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(base_job_name='cifar10',\n",
    "                       entry_point='cifar10_keras_sm.py',\n",
    "                       source_dir='training_script',\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters={'epochs' : 1},\n",
    "                       train_instance_count=1, train_instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train' :  'file://data/train',\n",
    "               'validation' :  'file://data/validation',\n",
    "               'eval' :  'file://data/eval'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time the estimator runs, it needs to download the container image from its Amazon ECR repository, but then training can begin immediately.  There’s no need to wait for a separate training cluster to be provisioned.  In addition, on subsequent runs, which may be necessary when iterating and testing, changes to your MXNet or TensorFlow script will start to run instantaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker\n",
    "In the next part, we'll use a GPU machine for faster training time\n",
    "First, We'll upload the data to S3. \n",
    "SageMaker creates a default bucket per region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a new SageMaker experiment\n",
    "Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare, and evaluate your machine learning experiments.\n",
    "\n",
    "Machine learning is an iterative process. You need to experiment with multiple combinations of data, algorithm and parameters, all the while observing the impact of incremental changes on model accuracy. Over time this iterative experimentation can result in thousands of model training runs and model versions. This makes it hard to track the best performing models and their input configurations. It’s also difficult to compare active experiments with past experiments to identify opportunities for further incremental improvements.\n",
    "\n",
    "Amazon SageMaker Experiments automatically tracks the inputs, parameters, configurations, and results of your iterations as trials. You can assign, group, and organize these trials into experiments. Experiments is integrated with Amazon SageMaker Studio providing a visual interface to browse your active and past experiments, compare trials on key performance metrics, and identify the best performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import time\n",
    "\n",
    "cifar10_experiment = Experiment.create(\n",
    "    experiment_name=\"TensorFlow-cifar10-experiment\",\n",
    "    description=\"Classification of cifar10 images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a new trial in this trial we'll run a simple 20 epochs training job on a GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create trial\n",
    "trial_name = f\"cifar10-training-job-{int(time.time())}\"\n",
    "trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=cifar10_experiment.experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new estimator (You can copy the command from above), this time use the **ml.p3.2xlarge** as the instance type and configure **epochs:20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, use the S3 data location for each of the channels\n",
    "```python\n",
    "dataset_location + '/train'\n",
    "dataset_location + '/validation' \n",
    "dataset_location + '/eval'\n",
    "```\n",
    "\n",
    "Connect the trial configured above to the job. add the experiment config to the fit function.\n",
    "```python\n",
    "experiment_config={\n",
    "                  \"ExperimentName\": cifar10_experiment.experiment_name, \n",
    "                  \"TrialName\": trial.trial_name,\n",
    "                  \"TrialComponentDisplayName\": \"Training\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train' :  'train_data_location',\n",
    "               'validation' :  'validation_data_location',\n",
    "               'eval' :  'eval_data_location'},\n",
    "             experiment_config=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good job!** \n",
    "You were able to run 20 epochs on a bigger instance in SageMaker.  \n",
    "Before continuing to the next notebook, take a look at the training jobs section in the SageMaker console, find your job and look at its configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
